#限制loss 含画图
import numpy as np
import matplotlib.pyplot as plt
#weight，bias是函数wTx+b的系数 代入sigmoid函数得到预测值 代入lossfunction得到loss

def sigmoid(model):
  """输入模型，输出概率值"""
  return 1/(1+np.exp(-model))

def modelPred(x, w, b):
  """输入模型参数 x，w，b 输出预测值y_pred"""
  return sigmoid(np.dot(x, w) + b)

def lossFunc(y, y_pred):
  """cross-entropy loss,y：实际值，y_pred:预测值 输出loss"""
  cel = - np.mean(y * np.log(y_pred) + (1 - y)*np.log(1 - y_pred)) 
  return cel

def getGradient(x, y, w, b, a):
  """计算损失函数关于w和b的梯度，
     输入x，w，b共同得到预测值 y为实际值 a为学习率 
     输出新的w，b值"""
  y_pred = modelPred(x, w, b)
  '''
  用矩阵求偏导的证明
  输入的多元变量 xT = [x1, x2, x3, ..., xn]
  现在我们有一个函数，该函数对多元变量的导数就是偏导:
  ∂f(x)/∂xT = [∂f(x)/∂x1, ∂f(x)/∂x2, ..., ∂f(x)/∂xn]
  假设该函数组为矩阵A，矩阵A作用于xT(两矩阵相乘)
  f(x) = [a1,1, a1,2, ..., a1,n] * [x1]
         [a2,1, a2,2, ..., a2,n]   [x2]
         .                         .
         .                         .
         .                         .
         [am,1, am,2, ..., am,n]   [xn]
  A*x  = [a1,1*x1 + a1,2*x2 + ... + a1,n*xn]
         [a2,1*x1 + a2,2*x2 + ... + a2,n*xn]
         .
         .
         .
         [am,1*x1 + am,2*x2 + ... + am,n*xn]
  对Ax进行求导，Ax对于变量x其中一个元素xi的偏导是：
  ∂Ax/∂xi = [∂Ax1/∂xi: a1,i]
            [∂Ax2/∂xi: a2,i]
            .
            .
            .
            [∂Axm/∂xi: am,i]
  整个求导则是对于每个元素xi求偏导结果的集合：
  ∂Ax/∂x = [∂Ax/∂x1, ∂Ax/∂x2, ..., ∂Ax/∂xn]
         = [[a1,1, a2,1, ..., am,1]T, [a1,2, a2,2, ..., am,2]T, ..., [a1,n, a2,n, ..., am,n]T]
         = AT
  矩阵A的偏导数是矩阵A的转置AT（由于矩阵的每个元素都是线性函数，偏导数是矩阵的转置。）
  --------------------------------
  x: 输入特征矩阵，维度为m×n。这里 m 是样本数量，n 是特征数量。
  y_pred: 模型的预测值，维度为 m×1，通常通过特征矩阵 x 与权重向量 w 相乘得到。
  y: 实际的目标值，维度为 m×1。
  m: 样本数量，通常用 len(y) 表示。w: 权重向量，维度为 n×1，表示每个特征对目标变量的影响程度。
  
  梯度其实就是偏导数的集合，代码中，gradient_w是权重的梯度，也就是是每个权重（与特征相关）的偏导数。
  基于特征的偏导数可以通过特征矩阵的转置xT计算(根据上面证明）。这个转置矩阵将每个特征的影响整合到一起，
  并作用于误差向量(y_pred-y)，通过矩阵乘法的方式计算每个特征对损失的贡献，最终得到权重的偏导数。
  --------------------------------
  '''
  dw = np.dot(x.T, (y_pred - y)) / (x.shape[0])
  db = np.sum(y_pred - y) / (x.shape[0])
  #梯度下降 向负方向更新
  w -= a * dw
  b -= a * db
  return w, b

# 示例数据
np.random.seed(42)
x = np.random.randn(100, 2)  # 100个样本，2个特征
true_w = np.array([3, -3])   # 真实权重
true_b = 0.5                 # 真实偏置
z = np.dot(x, true_w) + true_b
y = (z > 0).astype(int)      # 生成标签

def logisticRegression(x,y):
  """封装起来"""
  #初始值
  w = np.zeros(x.shape[1])
  b = 0                     
  a = 0.01
  epoch = 0
  loss = 1
  max_epoch = 20000
  epoch_loss = []
  #训练
  while loss >= 0.05 and epoch < max_epoch:
    w, b = getGradient(x, y, w, b, a)
    loss = lossFunc(y, modelPred(x, w, b))
    epoch_loss.append(loss)
    if epoch % 1000 == 0:
        print(f"Iteration {epoch}, Loss: {loss:.4f}")
    epoch += 1
  print(f"Weight: {w}, Bias: {b}")
  return epoch_loss,epoch



#画图
epoch_loss, epoch = logisticRegression(x, y)

if __name__ == "__main__":
    y_train_loss = epoch_loss  # Loss值，即y轴
    x_train_loss = range(len(y_train_loss))  # Loss的数量，即x轴

    plt.figure()

    # 去除顶部和右边框框
    ax = plt.axes()
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    plt.xlabel('epoch')  # x轴标签
    plt.ylabel('loss')  # y轴标签

    # 以x_train_loss为横坐标, y_train_loss为纵坐标，曲线宽度为1，实线，增加标签，训练损失
    plt.plot(x_train_loss, y_train_loss, linewidth=1, linestyle="solid", label="tra")
    plt.legend()
    plt.title('Loss curve')
    plt.show()
