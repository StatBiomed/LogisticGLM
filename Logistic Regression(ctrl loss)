#限制loss 含画图
import numpy as np
import matplotlib.pyplot as plt
#weight，bias是函数wTx+b的系数 代入sigmoid函数得到预测值 代入lossfunction得到loss

def sigmoid(model):
  """输入模型，输出概率值"""
  return 1/(1+np.exp(-model))

def modelPred(x, w, b):
  """输入模型参数 x，w，b 输出预测值y_pred"""
  return sigmoid(np.dot(x, w) + b)

def lossFunc(y, y_pred):
  """cross-entropy loss,y：实际值，y_pred:预测值 输出loss"""
  cel = - np.mean(y * np.log(y_pred) + (1 - y)*np.log(1 - y_pred)) 
  return cel

def getGradient(x, y, w, b, a):
  """计算损失函数关于w和b的梯度，
     输入x，w，b共同得到预测值 y为实际值 a为学习率 
     输出新的w，b值"""
  y_pred = modelPred(x, w, b)
  dw = np.dot(x.T, (y_pred - y)) / (x.shape[0])
  db = np.sum(y_pred - y) / (x.shape[0])
  #梯度下降 向负方向更新
  w -= a * dw
  b -= a * db
  return w, b

# 示例数据
np.random.seed(42)
x = np.random.randn(100, 2)  # 100个样本，2个特征
true_w = np.array([3, -3])   # 真实权重
true_b = 0.5                 # 真实偏置
z = np.dot(x, true_w) + true_b
y = (z > 0).astype(int)      # 生成标签

def logisticRegression(x,y):
  """封装起来"""
  #初始值
  w = np.zeros(x.shape[1])
  b = 0                     
  a = 0.01
  epoch = 0
  loss = 1
  max_epoch = 20000
  epoch_loss = []
  #训练
  while loss >= 0.05 and epoch < max_epoch:
    w, b = getGradient(x, y, w, b, a)
    loss = lossFunc(y, modelPred(x, w, b))
    epoch_loss.append(loss)
    if epoch % 1000 == 0:
        print(f"Iteration {epoch}, Loss: {loss:.4f}")
    epoch += 1
  print(f"Weight: {w}, Bias: {b}")
  return epoch_loss,epoch



#画图
epoch_loss, epoch = logisticRegression(x, y)

if __name__ == "__main__":
    y_train_loss = epoch_loss  # Loss值，即y轴
    x_train_loss = range(len(y_train_loss))  # Loss的数量，即x轴

    plt.figure()

    # 去除顶部和右边框框
    ax = plt.axes()
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    plt.xlabel('epoch')  # x轴标签
    plt.ylabel('loss')  # y轴标签

    # 以x_train_loss为横坐标, y_train_loss为纵坐标，曲线宽度为1，实线，增加标签，训练损失
    plt.plot(x_train_loss, y_train_loss, linewidth=1, linestyle="solid", label="tra")
    plt.legend()
    plt.title('Loss curve')
    plt.show()
